---
title: "Wikipedia Trends"
description: |
  This is the endproduct of a seminar on data visualization, showing views per day of a set of wikipedia articles.
author:
  - name: Aaron Peikert
    url: https://github.com/aaronpeikert
date: "`r Sys.Date()`"
output: radix::radix_article
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("fs")
library("tidyverse")
library("psych")
library("lubridate")
library("here")
library("tsibble")
library("pdc")
library("cowplot")
library("patchwork")
library("pageviews")
dir_walk(here("R"), source)
dir_walk(here("R"), knitr::read_chunk)
```

```{r packages2, eval=FALSE, include=FALSE}
if(!requireNamespace("pacman"))install.packages("pacman")
pacman::p_load("fs",
               "tidyverse",
               "psych",
               "lubridate",
               "here",
               "tsibble",
               "pdc",
               "cowplot",
               "patchwork",
               "ragg")
pacman::p_load_gh("Ironholds/pageviews@d32c629")
```

```{r, context="server"}
articles <- reactive(str_squish(flatten_chr(str_split(input$articles, ","))))
start <- reactive(ymd(input$start))
end <- reactive(ymd(input$end))
```

<p align="center">

```{r, layout = "l-screen-inset", echo=FALSE}
plotOutput("final")
```

<p/>

<p style="padding: 0 0 200px 0"></p>

```{r, context="server"}
views <- reactive(
  get_articles(articles(), start(), end()) %>%
    remove_outliers() %>%
    extract_trend() %>%
    add_dates() %>%
    extract_wday_trend() %>%
    extract_year_trend() %>%
    remove_pattern() %>%
    add_cluster(nclust = input$nclust)
)
```

```{r render, context="server", layout = "l-screen-inset", height = "600px"}
output$final <- renderPlot({
  plot_final(views())
}, execOnResize = TRUE, width = 1200, height = 600)
```

```{r input, echo=FALSE}
sliderInput("nclust", "Number of clusters:", 3, min = 1, max = 6)
textAreaInput("articles", "Articles:", "Alcohol, Nicotine, Caffeine, Cannabis, Methamphetamine, Cocaine, Heroin, Ecstasy, LSD, Xanax, Fentanyl, Percocet", "400px", "100px")
textInput("start", "Start date:", "2015-01-01")
textInput("end", "End date:", "2019-12-31")
```

# Backround Story 

The data is scraped via a rest api that is provided by wikipedia for this purpose. A thin wrapper around this restful api for R is developed by Oliver Keyes in the form of the [`pageview` package](https://github.com/Ironholds/pageviews/tree/d32c629c4589765e8c8d638a8a3f315481710120) (note that this project uses the github version fixed on the commit `d32c629`). [More information about the restfull api.](https://wikimedia.org/api/rest_v1/?doc)

I scrape a list of articles about several drugs, from the english wikipedia page from the first day of 2015 till the last of 2019.

```{r get-articles}
```

```{r}
psych_active <- c(
  "Alcohol",
  "Nicotine",
  "Caffeine",
  "Cannabis",
  "Methamphetamine",
  "Cocaine",
  "Heroin",
  "Ecstasy",
  "LSD",
  "Xanax",
  "Fentanyl",
  "Percocet"
)

start <- ymd("2015-01-01")
end <- ymd("2019-12-31")
views <- get_articles(psych_active, start, end)
```

Wikipedia seems to make some errors now and then (or more precisly there are sometimes issues with how articles are redirected). So I estimate the winsorized mean and standard deviation (sd) and remove (set to `NA`) values that lie beyond four (winsorized) sd above the (winsorized) mean. This estimation is pooled over all device types and agents.

```{r remove-outliers}
```

```{r}
views <- remove_outliers(views)
```

We have essentially three dimensions that are of interest and hence are target of the visualization: `date`, `article` and `number of views` (views from now on).

```{r}
views %>% 
  ggplot(aes(date, views, group = article, color = article)) +
  geom_line()
```

This first inspection reveals two interesting problems I will focus on --- beyond that this is not too pleasing graphic overall. First the average and variability of these articles views are quite different (ever heard about Percocet?). That warrents an aproach on how to scale. Second we have a rather high frequency that may hide some interesting patterns. The general approach to adress these issues is to decompose the signal into regular patterns most notably reucuring patterns and long term trends.

A first step to adress these two problems is to apply a smothing filter. To that end I will convert this into a timeseries format to apply rolling functions over the date. The rolling mean and sd over 365 days is used to approximate the long term differences between the articles averages and variabilities.

```{r extract-trends}
```

```{r}
views <- extract_trend(views)
```

```{r}
views %>% 
  ggplot(aes(date, trend, group = article, color = article)) +
  geom_line()
views %>% 
  ggplot(aes(date, trend_sd, group = article, color = article)) +
  geom_line()
```

Substrating the roling mean and dividing by the rolling sd results into a flexible z-standartisation. So lets plot the first graphic again.

```{r}
views %>% 
  ggplot(aes(date, views_std, group = article)) + geom_line(alpha = .3)
```

Besides that this plot is now uninformative about any differences between the articles, it gets clear that these articles share a not so small portion of their variance across date. So I particion the date variable into repeating date patterns.

```{r add-dates}
```

```{r}
views <- add_dates(views)
```

I think the some of the higher frequency comes from the weekday.

```{r extract-wday-trend}
```

```{r}
views <- extract_wday_trend(views)
```

```{r}
plot_wday_ <- views %>% 
  ggplot(aes(wday, views_wday, group = article, color = article)) +
  geom_line() + theme_minimal()
plot_wday_
```

This type of pattern is practically made for a polar coordinate system.

```{r}
plot_wday_ + coord_polar()
```

```{r extract-year-trend}
```

```{r}
views <- extract_year_trend(views)
```

```{r plot-year}
```

```{r}
plot_year(views)
```

After I have extracted the patterns I can remove them from the overall data and scale it back. Addtionally I create a 10 day smoothed version of the views without pattern.

```{r remove-pattern}
```

```{r}
views <- remove_pattern(views)
```

I create a plot that conveys both the raw data and the smoothed trend (smooth views without pattern +/- 1sd).

```{r plot-year}
```

```{r}
plot_year(views)
```

Till now I was in the business of discerning noise and pattern related to date in views. To summarise, this was done by implementing a randome effect model with  weekday and month as randome predictors based on a non parametric timevarient standatsation method. I won't go into detail about how many assumption this procedure makes and how many cannot be assumed to hold (most notably stationarity with regard to weekday and month or zero auto correlation). Unregarding this last point I think anything goes beyond this approach in terms of more sophisticated modelling will not improve the visual perception of this timeseries.

Another source of visual clutter is that we consider `r length(unique(views$article))` articles. That streches the boundary of what can be effectivly encoded in color. That is further compliceted by the fact that they are more or less simluar in shape. A possible remedy to that is to cluster the different articles into groups that are similar in their overall shape. How to define similarity in the context of timeseries is a complicated and well studies topic that is certainly beyond the scope of this project. However some important consideration may be noted. By chosing to scale the series, we deemed the differences between grand average and variability as uninteresting, which should be considered in how we operationlize similarity (because we only want similarity in thinks we consider important). Also because I have no domain knowledge about the articles under study and the long term goal of building an interactive dashboard we want a similarity definition that is parameter free. To my knowledge their is only one simularity measure that satisfies both: cross-entropy. Conceptually it may be understood as a measure that defines how much information the one timesiries caries over another, by assesing ow many bits may be saved when the one is used to compress the other and visa versa. This is formalized as the complexity of one measure plus the kulback leibler divergence between the two. While both terms of this sum are generally hard to compute some good approximation have been found. One especially suited to time series analysis is permutation distribution clustering. It takes the permutation distribution as a proxy for the required measure of complexity and the squared hellinger distance as a proxy for the kulback leiber divergence.

```{r cluster-ts}
```

```{r}
views <- add_cluster(views, 3)
```

Lets plot it crudely...

```{r}
plot_alltime(views) + facet_wrap(~cluster, scales = "free_y", ncol = 1)
```

Now we have three plots representing the reuccoring week and year pattern and the full timeline grouped into clusters.

I imagine a visual arrangement where each row represents a cluster and each column a timeframe. Lets beginn with the arrangement of a single row.

```{r}
cluster1 <- filter(views, cluster == 1)
plot_wday(cluster1) +
  plot_year(cluster1) +
  plot_alltime(cluster1) &
  theme(legend.position = "none")
```

And first I'll align the two polar plots, so they share a common y-axes.
