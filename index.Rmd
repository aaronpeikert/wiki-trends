---
title: "Wikipedia Trends"
description: |
  This is the endproduct of a seminar on data visualization, showing views per day of a set of wikipedia articles.
author:
  - name: Aaron Peikert
    url: https://github.com/aaronpeikert
date: "`r Sys.Date()`"
output: radix::radix_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = TRUE,
                      layout = "l-page",
                      dpi = 300)
if(!requireNamespace("pacman"))install.packages("pacman")
pacman::p_load("fs",
               "tidyverse",
               "psych",
               "lubridate",
               "here",
               "tsibble",
               "pdc",
               "cowplot",
               "patchwork",
               "ragg")
pacman::p_load_gh("Ironholds/pageviews@d32c629")
dir_walk(here("R"), source)
dir_walk(here("R"), knitr::read_chunk)
```

<iframe src="https://aaronpeikert.shinyapps.io/wiki-trends/" class="l-screen-inset" height="800px"></iframe>

# Background Story

The data is scraped via a rest API that is provided by Wikipedia for this purpose. Oliver Keyes develops a thin wrapper around this restful API for R in the form of the [`pageview` package](https://github.com/Ironholds/pageviews/tree/d32c629c4589765e8c8d638a8a3f315481710120) (note that this project uses the GitHub version fixed on the commit `d32c629`). [More information about the restful API.](https://wikimedia.org/api/rest_v1/?doc)
I scrape a list of articles about several drugs, from the English Wikipedia from the first day of 2015 till the last of 2019.

```{r get-articles}
```

```{r}
psych_active <- c(
  "Alcohol",
  "Nicotine",
  "Caffeine",
  "Cannabis",
  "Methamphetamine",
  "Cocaine",
  "Heroin",
  "Ecstasy",
  "Fentanyl"
)

start <- ymd("2015-01-01")
end <- ymd("2019-12-31")
views <- get_articles(psych_active, start, end)
```

Wikipedia seems to make some errors now and then (or more precisely there are sometimes issues with how articles are redirected). So I estimate the winsorized mean and standard deviation (sd) and remove (set to `NA`) values that lie beyond four (winsorized) sd above the (winsorized) mean. This estimation is pooled overall device types and agents.

```{r remove-outliers}
```

```{r}
views <- remove_outliers(views)
```

We have three dimensions that are of interest and hence are target of the visualization: `date`, `article` and `number of views` (views from now on).

```{r}
views %>% 
  ggplot(aes(date, views, group = article, color = article)) +
  geom_line()
```

This first inspection reveals two interesting problems I will focus on --- beyond that this is not too pleasing graphics overall. First, the average and variability of these articles views are quite different (ever heard about Fentanyl?). That warrants an approach on how to scale. Second, we have a rather high frequency that may hide some exciting patterns. The general approach to address these issues is to decompose the signal into regular patterns, most notably recurring patterns and long term trends.

A first step to address these two problems is to apply a smoothing filter. To that end, I will convert this into a time series format to apply rolling functions over the date. The rolling mean and sd over 365 days is used to approximate the long term differences between the articles averages and variabilities.

```{r extract-trends}
```

```{r}
views <- extract_trend(views)
```

```{r}
views %>% 
  ggplot(aes(date, trend, group = article, color = article)) +
  geom_line()
views %>% 
  ggplot(aes(date, trend_sd, group = article, color = article)) +
  geom_line()
```

Substrating the rolling mean and dividing by the rolling sd results into a flexible z-standardisation. So let's plot the first graphic again.

```{r}
views %>% 
  ggplot(aes(date, views_std, group = article)) + geom_line(alpha = .3)
```

Besides that this plot is now uninformative about any differences between the articles, it gets clear that these articles share a not so small portion of their variance across the date. So I partitioned the date variable into repeating date patterns.

```{r add-dates}
```

```{r}
views <- add_dates(views)
```

I think some of the higher frequency comes from differences over weekdays.

```{r extract-wday-trend}
```

```{r}
views <- extract_wday_trend(views)
```

```{r}
plot_wday_ <- views %>% 
  ggplot(aes(wday, views_wday, group = article, color = article)) +
  geom_line() + theme_minimal()
plot_wday_
```

This type of pattern is practically made for a polar coordinate system.

```{r}
plot_wday_ + coord_polar()
```

```{r extract-year-trend}
```

```{r}
views <- extract_year_trend(views)
```

```{r plot-year}
```

```{r}
plot_year(views)
```

Having extracted the patterns, I can remove them from the overall data and scale it back. Additionally, I create a ten-day smoothed version of the views without a pattern.

```{r remove-pattern}
```

```{r}
views <- remove_pattern(views)
```

I create a plot that conveys both the raw data and the smoothed trend (smooth views without pattern +/- 1sd).

```{r plot-year}
```

```{r}
plot_year(views)
```

Another source of visual clutter is that we consider 9 (possibly more) articles. That stretches the boundary of what can be effectively encoded in colour. That is further complicated by the fact that they are more or less similar in shape. A possible remedy is to cluster the different articles into groups that are similar in their overall shape. How to define similarity in the context of time-series is a complicated and well studies topic that is certainly beyond the scope of this project. However, some critical consideration may be noted. By choosing to scale the series, we deemed the differences between grand average and variability as uninteresting, which should be considered in how we operationalise similarity (because we only want similarity in thinks we consider essential). Also, because I have no domain knowledge about the articles under study and the long term goal of building an interactive dashboard, we want a similarity definition that is parameter-free. To my knowledge, there is only one similarity measure that satisfies both: cross-entropy. Conceptually it may be understood as a measure that defines how much information the one time series caries over another, by assessing how many bits may be saved when the one is used to compress the other and visa versa. This is formalised as the complexity of one measure plus the Kullback–Leibler divergence between the two. While both terms of this sum are generally hard to compute some good approximation, have been found. One solution uniquely suited to time series analysis is permutation distribution clustering. It takes the permutation distribution as a proxy for the required measure of complexity and the squared Hellinger distance as a proxy for the Kullback–Leibler divergence.

```{r cluster-ts}
```

```{r}
views <- add_cluster(views, 3)
clusters <- break_clusters(views)
```

Lets plot it crudely...

```{r}
plot_alltime(views) + facet_wrap(~cluster, scales = "free_y", ncol = 1)
```

Now we have three plots representing the reoccurring week and year pattern and the full timeline grouped into clusters.

I imagine a visual arrangement where each row represents a cluster and each column a timeframe — Lets begin with the arrangement of a single row.

```{r}
cluster1 <- filter(views, cluster == 1)
plot_wday(cluster1) +
  plot_year(cluster1) +
  plot_alltime(cluster1) &
  theme(legend.position = "none")
```

And first I'll align the polar plots, so they share a common y-axes.

```{r plot-helper}
```

```{r align-polar}
```

```{r}
polar <- prepare_polar(plot_wday(views),
                       plot_year(views),
                       clusters)
wrap_plots(flatten(polar), design = align_polar(polar))
```

Now the full timeline...

```{r align-alltime}
```

```{r}
alltime <- prepare_alltime(plot_alltime(views), clusters)
wrap_plots(alltime, design = align_alltime(alltime, 1, 2))
```

The colors are reused, thus creating visual relations which do not exist in the data. The color coding for this plot should optimize how well one can distinguish the different artices. A palette which varies the hue, but keeps luminance and chroma as fixed as possible is in my opinion a good fit for that task. However it is of special importance that they are as also as different as possible within each cluster. Because of the lack of a more sophisticated algorythm I scramble the order of the colors 100 times and take a sample with the highest sum of squares of hue within each group.

```{r color}
```

But a legend is still missing.

```{r add-legend}
```

So the final Plot is:

```{r final-plot}
```

```{r, layout = "l-screen-inset", fig.width = 16, fig.height = 9}
plot_final(views)
```

# Shortcomings

Even after fidelling around quite a while I was unable to solve two problems, first the x-axis of the polar plots puts the first and last value on the same angle, second there is no axis title on the two y-axises which spans the whole plot (I head to revert to the caption for the information on the units). I gave up on these, because ["[...] `ggplot2` in many ways seems to be fueled by magic and unicorn blood [...]"](https://www.data-imaginist.com/2017/beneath-the-canvas/). Another anoying thing is the speed of plotting, which is something I wouldn't even dare to try to improve.

If I were to recreate this plot I would do it in D3.js, to have finer control over such details and to enable real interactiveness, which is currently limited by beforementioned speed and principle design limitations of `ggplot`.
